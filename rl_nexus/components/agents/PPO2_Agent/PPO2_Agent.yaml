component: agents/PPO2_Agent
model:
  component: models/Simple_AC_Model
local_mode: False               # Code should be executed serially which is useful for debugging
num_workers: 2                  # Number of rollout worker actors, or 0 for the trainer actor.
num_envs_per_worker: 1          # Use >1 for inference-bottlednecked workloads.
batch_mode: truncate_episodes   # "complete_episodes" (lower variance) or "truncate_episodes" (evenly-sized batches).
num_gpus: 0                     # Number of training GPUs. Can be fractional.
optimizer: {}                   # Arguments passed to the specific policy optimizer.
num_cpus_per_worker: 1          # CPUs per worker. Can be fractional.
num_gpus_per_worker: 0          # GPUs per worker. Can be fractional.
gamma: 0.99                     # Discount factor.
horizon: null                   # Override of the environment's max steps per episode.
soft_horizon: False             # If True, env is not reset at the horizon.
no_done_at_end: False           # If True, done is not set at end of episode.
normalize_actions: False        # If True, unsquash actions to space bounds.
clip_rewards: null              # True to clip rewards to experience postprocessing.
clip_actions: True              # Whether to np.clip() actions.
preprocessor_pref: rllib        # Whether to default to rllib or deepmind preprocessors.
use_critic: True                # True to use critic baseline. Required for GAE.
use_gae: True                   # Use Generalized Advantage Estimator
lambda: 1.0                     # GAE(gamma) parameter
kl_coeff: 0.2                   # Initial coefficient for KL divergence.
rollout_fragment_length: 200    # Number of time steps in each training rollout.
train_batch_size: 4000          # Time steps per training batch. (>= rollout_fragment_length)
sgd_minibatch_size: 128         # Minibatch size within each epoch.
shuffle_sequences: True         # Whether to shuffle sequences in the batch when training (recommended).
num_sgd_iter: 30                # Number of SGD iterations in each outer loop (number of epochs to execute per train batch).
lr: 5.0e-5                      # Learning rate.
lr_schedule: null               # Learning rate schedule
vf_loss_coeff: 1.0              # Value Function Loss coefficient
entropy_coeff: 0.0              # Entropy coefficient
entropy_coeff_schedule: null    # Decay schedule for the entropy regularizer.
clip_param: 0.3                 # PPO clip parameter.
vf_clip_param: 10.0             # Clip param for the value function.
grad_clip: null                 # Amount to clip the global norm of gradients.
kl_target: 0.01                 # Target value for KL divergence.
observation_filter: NoFilter    # Observation filter to apply to the observation.
simple_optimizer: False         # Use sync samples optimizer instead of multi-gpu one
metrics_smoothing_episodes: 100 # Rllib metrics are smoothed over this many episodes.
seed: 123                       # The first worker's random seed. Override with randint to make non-deterministic.
vf_share_layers: False
# --- rllib_config settings ---
# === Settings for Rollout Worker processes ===
# Number of rollout worker actors to create for parallel sampling. Setting
# this to 0 will force rollouts to be done in the trainer actor.
#num_workers: 2
# Number of environments to evaluate vectorwise per worker. This enables
# model inference batching, which can improve performance for inference
# bottlenecked workloads.
#num_envs_per_worker: 1
# Divide episodes into fragments of this many steps each during rollouts.
# Sample batches of this size are collected from rollout workers and
# combined into a larger batch of `train_batch_size` for learning.
#
# For example, given rollout_fragment_length=100 and train_batch_size=1000:
#   1. RLlib collects 10 fragments of 100 steps each from rollout workers.
#   2. These fragments are concatenated and we perform an epoch of SGD.
#
# When using multiple envs per worker, the fragment size is multiplied by
# `num_envs_per_worker`. This is since we are collecting steps from
# multiple envs in parallel. For example, if num_envs_per_worker=5, then
# rollout workers will return experiences in chunks of 5*100 = 500 steps.
#
# The dataflow here can vary per algorithm. For example, PPO further
# divides the train batch into minibatches for multi-epoch SGD.
#rollout_fragment_length: 20
# Whether to rollout "complete_episodes" or "truncate_episodes" to
# `rollout_fragment_length` length unrolls. Episode truncation guarantees
# evenly sized batches, but increases variance as the reward-to-go will
# need to be estimated at truncation boundaries.
#batch_mode: truncate_episodes

# === Settings for the Trainer process ===
# Number of GPUs to allocate to the trainer process. Note that not all
# algorithms can take advantage of trainer GPUs. This can be fractional
# (e.g., 0.3 GPUs).
#num_gpus: 0
# Training batch size, if applicable. Should be >= rollout_fragment_length.
# Samples batches will be concatenated together to a batch of this size,
# which is then passed to SGD.
#train_batch_size: 200
# Arguments to pass to the policy optimizer. These vary by optimizer.
#optimizer: {}

# === Advanced Resource Settings ===
# Number of CPUs to allocate per worker.
#num_cpus_per_worker: 1
# Number of GPUs to allocate per worker. This can be fractional. This is
# usually needed only if your env itself requires a GPU (i.e., it is a
# GPU-intensive video game), or model inference is unusually expensive.
#num_gpus_per_worker: 0

# === Environment Settings ===
# Discount factor of the MDP.
#gamma: 0.99
# Number of steps after which the episode is forced to terminate. Defaults
# to `env.spec.max_episode_steps` (if present) for Gym envs.
#horizon: null
# Calculate rewards but don't reset the environment when the horizon is
# hit. This allows value estimation and RNN state to span across logical
# episodes denoted by horizon. This only has an effect if horizon != inf.
#soft_horizon: False
# Don't set 'done' at the end of the episode. Note that you still need to
# set this if soft_horizon=True, unless your env is actually running
# forever without returning done=True.
#no_done_at_end: False
# Unsquash actions to the upper and lower bounds of env's action space
#normalize_actions: False
# Whether to clip rewards prior to experience postprocessing. Setting to
# None means clip for Atari only.
#clip_rewards: null
# Whether to np.clip() actions to the action space low/high range spec.
#clip_actions: True
# Whether to use rllib or deepmind preprocessors by default
#preprocessor_pref: rllib

# === PPO Settings ===
# Should use a critic as a baseline (otherwise don't use value baseline;
# required for using GAE).
#use_critic: True
# If true, use the Generalized Advantage Estimator (GAE)
# with a value function, see https://arxiv.org/pdf/1506.02438.pdf.
#use_gae: True
# The GAE(lambda) parameter.
#lambda: 1.0
# Initial coefficient for KL divergence.
#kl_coeff: 0.2
# Size of batches collected from each worker.
#rollout_fragment_length: 200
# Number of timesteps collected for each SGD round. This defines the size
# of each SGD epoch.
#train_batch_size: 4000
# Total SGD batch size across all devices for SGD. This defines the
# minibatch size within each epoch.
#sgd_minibatch_size: 200
# Whether to shuffle sequences in the batch when training (recommended).
#shuffle_sequences: True
# Number of SGD iterations in each outer loop (i.e., number of epochs to
# execute per train batch).
#num_sgd_iter: 30
# Stepsize of SGD.
#lr: 5.0e-5
# Learning rate schedule.
#lr_schedule: null
# Coefficient of the value function loss. IMPORTANT: you must tune this if
# you set vf_share_layers: True.
#vf_loss_coeff: 1.0
# Coefficient of the entropy regularizer.
#entropy_coeff: 0.0
# Decay schedule for the entropy regularizer.
#entropy_coeff_schedule: null
# PPO clip parameter.
#clip_param: 0.3
# Clip param for the value function. Note that this is sensitive to the
# scale of the rewards. If your expected V is large, increase this.
#vf_clip_param: 10.0
# If specified, clip the global norm of gradients by this amount.
#grad_clip: null
# Target value for KL divergence.
#kl_target: 0.01
# Which observation filter to apply to the observation.
#observation_filter: NoFilter
# Uses the sync samples optimizer instead of the multi-gpu one. This is
# usually slower, but you might want to try it if you run into issues with
# the default optimizer.
#simple_optimizer: False

# === Advanced Rollout Settings ===
# Number of previous episodes that metrics are averaged across
#metrics_smoothing_episodes: 100
# This argument, in conjunction with worker_index, sets the random seed of
# each worker, so that identically configured trials will have identical
# results. This makes experiments reproducible.
# Override with randint for randomly generated seeds, or set to null to use
# without a seed
#seed: 123
