component: processors/OPE_E2E
enabled: 1
dataset_seed: randint

data_collector:
  component: processors/Data_Collection
  enabled: true
  behavior_policy_range:
    min: null
    max: null
    step: null
  temperature: 1.0
  num_episodes: 100
  save_data_to: $datastore/ope/data/<name>/
  dataset_seed: null
    # start: null
    # stop: null
    # step: null
  load_model_from: $datastore/ope/policy/<name>/rl/ 
  environment:
    component: environments/<environment>
    name: <name>
    seed: null
    fixed_length_episode: true
    max_ep_len: <horizon>
    smooth_reward: false
    stochastic_dynamics: false
  model:
    component: models/Simple_Policy_Model
    fcnet_hiddens: [64,64]
    fcnet_activation: tanh


off_policy_estimator:
  component: processors/Off_Policy_Evaluation
  enabled: true
  dataset_seed: null
  use_ray: false
  algo_seed: randint
  behavior_policy_range:
    min: null
    max: null
  load_data_from: null
  load_model_from: null
  save_results_to: null
  target_policy_id: null
  target_policy_temperature: 1.0
  num_episodes: 100
  horizon: 200
  gamma: 0.99
  normalization: std_norm
  environment:
    component: environments/<environment>
    name: <name>
    seed: null
    fixed_length_episode: false
    max_ep_len: <horizon>
    smooth_reward: false
    stochastic_dynamics: false
  model:
    component: models/Simple_Policy_Model
    fcnet_hiddens: [64,64]
    fcnet_activation: tanh

  # on_policy_eval_num_episodes: 100
  # offline_estimators: ['PDIS', 'WPDIS', 'MB', 'LSTD', 'LSTDQ', 'TDREG', 'MWL', 'MSWL', 'MQL', 'DualDICE']

  # ## Hyperparameters for different estimators listed below
  # MSWL:
  #   k_tau: 1.0
  #   lr: 5.0e-3
  #   reg: 0.0
  #   batch_size: 500
  #   num_iter: 5000
  #   hidden_layers: [32,32]
  #   eval_interval: 100
  #   tail_average: 10

  # MWL:
  #   k_tau: 3.0
  #   lr: 5.0e-3
  #   reg: 0.0
  #   batch_size: 500
  #   num_iter: 10000
  #   hidden_layers: [32,32]
  #   eval_interval: 100
  #   tail_average: 10

  # MQL:
  #   k_tau: 15
  #   lr: 5.0e-3
  #   reg: 2.0e-3
  #   batch_size: 500
  #   num_iter: 10000 #30000 original implementation
  #   hidden_layers: [32,32]
  #   eval_interval: 100
  #   tail_average: 10

  # DualDICE:
  #   nu_learning_rate: 0.0005
  #   zeta_learning_rate: 0.005
  #   batch_size: 500
  #   num_iter: 10000 #30000 original implementation
  #   log_every: 100
  #   function_exponent: 1.5
  #   tail_average: 10
  #   hidden_dim: 32
  #   hidden_layers: 2
  #   activation: relu

  # Kernel:
  #   model_reg: 1.0e-3
  #   reward_reg: 1.0e-3
  #   value_reg: 1.0e-3
  #   num_random_feature_per_obs_dim: 250
  #   default_length_scale: 0.1
  #   scale_length_adjustment: median

  # TDREG:
  #   value_reg: 1.0e-3
  #   input_mode: sa # or s (depending on whether we want the input to take the action as well)
  #   num_iter: 500
  #   batch_size: 1000
  #   lr: 1.0e-3
  #   td_ball_epsilon: 1.0e-1
  #   normalize_w: true
  #   hidden_layers: [64,64]
  #   activation: tanh
  #   w_reg: 1.0e-5
  #   num_random_feature_per_obs_dim: 250
  #   default_length_scale: 0.1
  #   scale_length_adjustment: median
  #   use_var_in_loss: false
  #   tail_average: 10
